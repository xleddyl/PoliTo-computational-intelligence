{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Policy Search\n",
    "\n",
    "## Task\n",
    "\n",
    "Write agents able to play [_Nim_](https://en.wikipedia.org/wiki/Nim), with an arbitrary number of rows and an upper bound $k$ on the number of objects that can be removed in a turn (a.k.a., _subtraction game_).\n",
    "\n",
    "The player **taking the last object wins**.\n",
    "\n",
    "-  Task3.1: An agent using fixed rules based on _nim-sum_ (i.e., an _expert system_)\n",
    "-  Task3.2: An agent using evolved rules\n",
    "-  Task3.3: An agent using minmax\n",
    "-  Task3.4: An agent using reinforcement learning\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âš™ï¸ Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from collections import namedtuple\n",
    "import random\n",
    "import math\n",
    "from typing import Callable\n",
    "from copy import deepcopy\n",
    "from itertools import accumulate\n",
    "from operator import xor\n",
    "\n",
    "logging.getLogger().setLevel(logging.DEBUG)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ§© _Nim_ and _Nimply_ classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nimply = namedtuple(\"Nimply\", \"row, num_objects\")\n",
    "\n",
    "\n",
    "class Nim:\n",
    "    def __init__(self, num_rows: int, k: int = None) -> None:\n",
    "        self._rows = [i * 2 + 1 for i in range(num_rows)]\n",
    "        self._k = k\n",
    "\n",
    "    def __bool__(self):\n",
    "        return sum(self._rows) > 0\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"[{' '.join(str(i) for i in self.rows)}]\"\n",
    "\n",
    "    @property\n",
    "    def rows(self) -> tuple:\n",
    "        return tuple(self._rows)\n",
    "\n",
    "    @property\n",
    "    def k(self) -> int:\n",
    "        return self._k\n",
    "\n",
    "    def nimming(self, ply: Nimply) -> None:\n",
    "        row, num_objects = ply\n",
    "        assert self._rows[row] >= num_objects\n",
    "        assert self._k is None or num_objects <= self._k\n",
    "        assert num_objects != 0\n",
    "        self._rows[row] -= num_objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cook_status(state: Nim) -> dict:\n",
    "    cooked = dict()\n",
    "    cooked[\"possible_moves\"] = [\n",
    "        (r, o) for r, c in enumerate(state.rows) for o in range(1, c + 1) if state.k is None or o <= state.k\n",
    "    ]\n",
    "    cooked[\"active_rows_number\"] = sum(o > 0 for o in state.rows)\n",
    "    cooked[\"active_rows\"] = [(i, o) for i, o in enumerate(state.rows) if o > 0]\n",
    "    cooked[\"shortest_row\"] = min((x for x in enumerate(\n",
    "        state.rows) if x[1] > 0), key=lambda y: y[1])[0]\n",
    "    cooked[\"longest_row\"] = max(\n",
    "        (x for x in enumerate(state.rows)), key=lambda y: y[1])[0]\n",
    "    cooked[\"nim_sum\"] = nim_sum(state)\n",
    "\n",
    "    brute_force = list()\n",
    "    for m in cooked[\"possible_moves\"]:\n",
    "        tmp = deepcopy(state)\n",
    "        tmp.nimming(m)\n",
    "        brute_force.append((m, nim_sum(tmp)))\n",
    "    cooked[\"brute_force\"] = brute_force\n",
    "    return cooked\n",
    "\n",
    "\n",
    "def nim_sum(state: Nim) -> int:\n",
    "    *_, result = accumulate(state.rows, xor)\n",
    "    return result\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ•¹ï¸ Strategies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Optimal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_strategy(state: Nim) -> Nimply:\n",
    "    data = cook_status(state)\n",
    "    tmp = next((bf for bf in data[\"brute_force\"] if bf[1] == 0), random.choice(\n",
    "        data[\"brute_force\"]))[0]\n",
    "    return Nimply(*tmp)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_strategy(state: Nim) -> Nimply:\n",
    "    row = random.choice([r for r, c in enumerate(state.rows) if c > 0])\n",
    "    num_objects = random.randint(\n",
    "        1, state.rows[row] if state.k is None else min(state.rows[row], state.k))\n",
    "    return Nimply(row, num_objects)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  3.1 - Fixed rules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fixed_strategy(state: Nim) -> Nimply:\n",
    "    cooked = cook_status(state)\n",
    "\n",
    "    # endgame strategy\n",
    "    if cooked[\"active_rows_number\"] == 3:\n",
    "        rows = cooked[\"active_rows\"]\n",
    "        dangerRow = [t for t in rows if t[1] == 1]\n",
    "        if len(dangerRow) == 1:\n",
    "            return Nimply(dangerRow[0][0], 1)\n",
    "\n",
    "    # main strategy\n",
    "    if cooked[\"active_rows_number\"] % 2 == 0:\n",
    "        row = cooked[\"longest_row\"]\n",
    "        num_objects = 1\n",
    "    else:\n",
    "        row = cooked[\"longest_row\"]\n",
    "        num_objects = state.rows[row]\n",
    "\n",
    "    if state.k is not None:\n",
    "        num_objects = min(num_objects, state.k)\n",
    "\n",
    "    return Nimply(row, num_objects)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  âš ï¸ 3.2 - Evolved rules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "POP_SIZE = 10\n",
    "GEN = 100\n",
    "OFF_SIZE = 10\n",
    "\n",
    "\n",
    "def evolved_strategy(state: Nim) -> Nimply:\n",
    "    Individual = namedtuple('Individual', ('genome', 'fitness'))\n",
    "\n",
    "    def mutation(g):\n",
    "        point = random.randint(0, len(g) - 1)\n",
    "        return g[:point] + (1 - g[point,]) + g[point + 1:]\n",
    "\n",
    "    def crossover(g1, g2):\n",
    "        cut = random.randint(0, min(len(g1), len(g2)) - 1)\n",
    "        return g1[:cut] + g2[cut:]\n",
    "\n",
    "    def tournament(population, size):\n",
    "        return max(random.choices(population, k=size), key=lambda i: i.fitness)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  3.3 - MinMax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DEPTH = 5\n",
    "\n",
    "\n",
    "def minmax_strategy(state: Nim) -> Nimply:\n",
    "\n",
    "    def minmax(state: Nim, turn: int, alpha: float = -1, beta: float = 1, depth: int = 0) -> tuple:\n",
    "        if not state or (MAX_DEPTH is not None and depth >= MAX_DEPTH):\n",
    "            return None, turn\n",
    "\n",
    "        cooked = cook_status(state)\n",
    "        moves = cooked[\"possible_moves\"]\n",
    "        score = turn * math.inf\n",
    "        for ply in moves:\n",
    "            new_state = deepcopy(state)\n",
    "            new_state.nimming(ply)\n",
    "            _, val = minmax(new_state, -turn, alpha, beta, depth + 1)\n",
    "            score = (min if turn == 1 else max)(score, val)\n",
    "            if turn == 1:\n",
    "                if score <= alpha:\n",
    "                    break\n",
    "                beta = min(beta, score)\n",
    "            if turn == -1:\n",
    "                if score >= beta:\n",
    "                    break\n",
    "                alpha = max(alpha, score)\n",
    "        return (ply, score)\n",
    "\n",
    "    ply, score = minmax(state, 1)\n",
    "    return Nimply(*ply)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  3.4 - Reinforcement learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 500\n",
    "ALPHA = 0.1\n",
    "RANDOM_FACTOR = 0.2\n",
    "TEACHER = optimal_strategy\n",
    "\n",
    "\n",
    "def rl_strategy(state: Nim) -> Nimply:\n",
    "\n",
    "    class RLAgent(object):\n",
    "        def __init__(self, alhpa=0.1, random_factor=0.2):\n",
    "            self.state_history = []\n",
    "            self.alpha = alhpa\n",
    "            self.random_factor = random_factor\n",
    "            self.G = {}\n",
    "\n",
    "        def choose_action(self, state):\n",
    "            cooked = cook_status(state)\n",
    "            moves = cooked[\"possible_moves\"]\n",
    "            maxG = -10e15\n",
    "            randomN = random.random()\n",
    "\n",
    "            if randomN < self.random_factor:\n",
    "                next_move = random.choice(moves)\n",
    "                new_state = deepcopy(state)\n",
    "                new_state.nimming(next_move)\n",
    "                if new_state.rows not in self.G:\n",
    "                    self.G[new_state.rows] = random.uniform(0.1, 1.0)\n",
    "            else:\n",
    "                next_move = random.choice(moves)\n",
    "                for action in moves:\n",
    "                    new_state = deepcopy(state)\n",
    "                    new_state.nimming(action)\n",
    "                    if new_state.rows not in self.G:\n",
    "                        self.G[new_state.rows] = random.uniform(0.1, 1.0)\n",
    "                    if self.G[new_state.rows] >= maxG:\n",
    "                        next_move = action\n",
    "                        maxG = self.G[new_state.rows]\n",
    "\n",
    "            return next_move\n",
    "\n",
    "        def update_state_history(self, state, reward):\n",
    "            self.state_history.append((state, reward))\n",
    "\n",
    "        def learn(self):\n",
    "            target = 0\n",
    "\n",
    "            for prev, reward in reversed(self.state_history):\n",
    "                self.G[prev] = self.G[prev] + \\\n",
    "                    self.alpha * (target - self.G[prev])\n",
    "                target += reward\n",
    "\n",
    "            self.state_history = []\n",
    "            self.random_factor -= 10e-5\n",
    "\n",
    "    agent = RLAgent(ALPHA, RANDOM_FACTOR)\n",
    "    for i in range(EPOCHS):\n",
    "        copy_state = deepcopy(state)\n",
    "        player = random.choice([0, 1])\n",
    "        while copy_state:\n",
    "            if player == 0:\n",
    "                ply = agent.choose_action(copy_state)\n",
    "                copy_state.nimming(ply)\n",
    "                agent.update_state_history(\n",
    "                    copy_state.rows, 0 if copy_state else 1)\n",
    "            else:\n",
    "                ply = TEACHER(copy_state)\n",
    "                copy_state.nimming(ply)\n",
    "            player = 1 - player\n",
    "        agent.learn()\n",
    "    return Nimply(*agent.choose_action(state))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ† Single Match\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NIM_SIZE = 5\n",
    "K = None\n",
    "\n",
    "players = (\n",
    "    minmax_strategy,\n",
    "    optimal_strategy,\n",
    ")\n",
    "\n",
    "\n",
    "def match(players: tuple[Callable, Callable]) -> Callable:\n",
    "    nim = Nim(NIM_SIZE, K)\n",
    "    player = 0\n",
    "    logging.debug(f\"  -  {nim}\")\n",
    "\n",
    "    while nim:\n",
    "        ply = players[player](nim)\n",
    "        nim.nimming(ply)\n",
    "        logging.debug(\n",
    "            f\"  -  {players[player].__name__} -> row: {ply.row} / obj: {ply.num_objects}\")\n",
    "        logging.debug(f\"  -  {nim}\")\n",
    "        player = 1 - player\n",
    "    winner = 1 - player\n",
    "    return players[winner]\n",
    "\n",
    "\n",
    "winner = match(players)\n",
    "logging.info(f\" {winner.__name__} won!\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Š Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NIM_SIZE = 5\n",
    "K = None\n",
    "NUM_MATCHES = 100\n",
    "\n",
    "players = (\n",
    "    minmax_strategy,\n",
    "    rl_strategy,\n",
    ")\n",
    "\n",
    "\n",
    "def evaluate(players: tuple[Callable, Callable]) -> int:\n",
    "    won = 0\n",
    "    for m in range(NUM_MATCHES):\n",
    "        nim = Nim(NIM_SIZE, K)\n",
    "        player = random.choice([0, 1])\n",
    "        while nim:\n",
    "            ply = players[player](nim)\n",
    "            nim.nimming(ply)\n",
    "            player = 1 - player\n",
    "        if player == 1:\n",
    "            won += 1\n",
    "    return won\n",
    "\n",
    "\n",
    "won = evaluate(players)\n",
    "rate = won / NUM_MATCHES * 100\n",
    "logging.info(\n",
    "    f\"  -  the winning rate of {players[0].__name__} against {players[1].__name__} was {rate}% ({won}/{NUM_MATCHES})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97cc609b13305c559618ec78a438abc56230b9381f827f22d070313b9a1f3777"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
